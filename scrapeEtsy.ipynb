{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests as rqst\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define useragent and url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseurl = 'https://www.etsy.com/'\n",
    "user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "\n",
    "# Set the headers to include the user agent\n",
    "headers = {\n",
    "    'User-Agent': user_agent,\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "}\n",
    "\n",
    "# Start with the first page\n",
    "page_num = 1\n",
    "\n",
    "# Keep track of all the product data\n",
    "product_data = []\n",
    "\n",
    "# Keep track of the total time taken\n",
    "total_time = 0\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Input code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while True:\n",
    "#     print('In which field you want to analyze? Please write:\\n - search\\n - market\\n')\n",
    "#     analyze_preference = input().lower()\n",
    "    \n",
    "#     if analyze_preference == 'search':\n",
    "#         print('Please provide the search term')\n",
    "#         search_term=input().replace(' ','+')\n",
    "#         break\n",
    "#     elif analyze_preference == 'market':\n",
    "#         print('Please provide the search term')\n",
    "#         search_term=input().replace(' ','_')\n",
    "#         break\n",
    "#     else:\n",
    "#         print('Invalid input. Please enter \"search\" or \"market\".\\n')\n",
    "\n",
    "# print(search_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term='budget planner'\n",
    "search_term=search_term.replace(' ','+')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For market search https://www.etsy.com/market/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while True:\n",
    "#     # Construct the URL for the current page\n",
    "#     url = f'https://www.etsy.com/market/{search_term}?ref=pagination&page={page_num}'\n",
    "\n",
    "#     # Send a request to the page\n",
    "#     try:\n",
    "#         start_time = time.time()\n",
    "#         r = rqst.get(url, headers=headers)\n",
    "#         r.raise_for_status()\n",
    "#         end_time = time.time()\n",
    "#         elapsed_time = end_time - start_time\n",
    "#         total_time += elapsed_time\n",
    "#         print(f\"Page {page_num} scraped in {elapsed_time:.2f} seconds\")\n",
    "#     except rqst.exceptions.HTTPError as err:\n",
    "#         if err.response.status_code == 404:\n",
    "#             #print(f\"404 Error: Page {page_num} not found\")\n",
    "#             break\n",
    "#         else:\n",
    "#             raise\n",
    "\n",
    "#     # Parse the HTML content of the page\n",
    "#     soup = BeautifulSoup(r.content, 'lxml')\n",
    "\n",
    "#     # Find all the product listings on the page\n",
    "#     listings = soup.find('ul', {'id': 'search-results'})\n",
    "#     product_listings = listings.find_all('li')\n",
    "\n",
    "#     # Extract the required data from each listing and append to the product_data list\n",
    "#     for listing in product_listings:\n",
    "#         title_elem = listing.select_one('h2.v2-listing-card__title')\n",
    "#         title = title_elem.text.strip()\n",
    "#         price_elem = listing.select_one('span.currency-value')\n",
    "#         price = price_elem.text.strip()\n",
    "#         review_elem = listing.select_one('span.wt-nudge-l-3')\n",
    "#         review = review_elem.text.strip() if review_elem else 'N/A'\n",
    "#         ad_etsy_seller = bool(listing.select_one('span:-soup-contains(\"by Etsy seller\")'))\n",
    "#         bestseller = bool(listing.select_one('span:-soup-contains(\"Bestseller\")'))\n",
    "#         starseller = bool(listing.select_one('p.star-seller-badge-lavender-text-light'))\n",
    "#         product_url_elem = listing.select_one('a.listing-link')\n",
    "#         product_url = product_url_elem.get('href') if product_url_elem else 'N/A'\n",
    "\n",
    "\n",
    "#         # Add the extracted data as a dictionary to the product_data list\n",
    "#         product_data.append({\n",
    "#             'StarSeller': starseller,\n",
    "#             'Title': title,\n",
    "#             'Price': price,\n",
    "#             'Review': review,\n",
    "#             'Etsy_AD': ad_etsy_seller,\n",
    "#             'BestSeller': bestseller,\n",
    "#             'URL': product_url\n",
    "#         })\n",
    "\n",
    "#     # Increment the page number\n",
    "#     page_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    # Construct the URL for the current page\n",
    "    url = f'https://www.etsy.com/search?q={search_term}&ref=pagination&page={page_num}&currency=USD'\n",
    "\n",
    "    # Send a request to the page\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        r = rqst.get(url, headers=headers)\n",
    "        r.raise_for_status()\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        total_time += elapsed_time\n",
    "        print(f\"Page {page_num} scraped in {elapsed_time:.2f} seconds\")\n",
    "    except rqst.exceptions.HTTPError as err:\n",
    "        if err.response.status_code == 404:\n",
    "            #print(f\"404 Error: Page {page_num} not found\")\n",
    "            break\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(r.content, 'lxml')\n",
    "\n",
    "    # Find all the product listings on the page\n",
    "    page_listings = soup.find_all('div', class_='v2-listing-card')\n",
    "\n",
    "    # Count the number of listings found on the page\n",
    "    num_products_per_page = len(page_listings)\n",
    "\n",
    "    print(f'Number of products scraped: {num_products_per_page}')\n",
    "\n",
    "    # Extract the required data from each listing and append to the product_data list\n",
    "    for listing in page_listings:\n",
    "        # Extract the product title\n",
    "        currency=listing.find('span', class_='currency-symbol').text.strip()\n",
    "        title = listing.find('h3', class_='v2-listing-card__title').text.strip()\n",
    "        price= listing.select_one('span.currency-value').text.strip()\n",
    "        review_elem = listing.select_one('span.wt-nudge-l-3')\n",
    "        review = review_elem.text.strip() if review_elem else 'N/A'\n",
    "        ad_etsy_seller = bool(listing.select_one('span:-soup-contains(\" Advertisement by Etsy seller\")'))\n",
    "        bestseller = bool(listing.select_one('span:-soup-contains(\"Bestseller\")'))\n",
    "        starseller = bool(listing.select_one('p.star-seller-badge-lavender-text-light'))\n",
    "        product_url = listing.select_one('a.listing-link').get('href')\n",
    "        # Add the extracted data as a dictionary to the product_data list\n",
    "        product_data.append({\n",
    "            'StarSeller': starseller,\n",
    "            'Title': title,\n",
    "            'Price': price,\n",
    "            'Review': review,\n",
    "            'Etsy_AD': ad_etsy_seller,\n",
    "            'BestSeller': bestseller,\n",
    "            'Currency': currency,\n",
    "            'URL': product_url\n",
    "        })\n",
    "\n",
    "    page_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the total time taken\n",
    "print(f\"Total time taken: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pandas DataFrame from the product_data list\n",
    "df = pd.DataFrame(product_data)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the DataFrame to a CSV file\n",
    "df.to_csv('etsy_products.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADS not working, get into the produyct and fetch the review for that product"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
